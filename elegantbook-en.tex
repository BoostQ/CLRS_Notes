\documentclass[11pt,toc=twocol]{elegantbook}
\usepackage{import}
\usepackage{arydshln}
\usepackage{indentfirst}

\setcounter{tocdepth}{2}


\title{CLRS Notes}
\subtitle{MIT 6.046J}

\author{Haopeng Li}
%\institute{Elegant\LaTeX{} Program}
\date{\today}
%\bioinfo{Bio}{Information}

%\extrainfo{Victory won\rq t come to us unless we go to it. }

\logo{logo-blue.png}
\cover{cover.jpg}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}


\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Analysis of Algorithms}
\begin{introduction}
  \item Insertion sort
  \item Asymptotic analysis
  \item Merge sort
  \item Recurrences
\end{introduction}
\begin{definition}[Algorithms]
  The theoretical study of computer-program performance and resource usage.
\end{definition}

\begin{note}
  Why study algorithms and performance?
  \begin{itemize}
    \item Algorithms help us to understand scalability.
    \item Performance often draws the line between what is feasible and what is impossible.
    \item Algorithmic mathematics provides a language for talking about program behavior.
    \item Performance is the currency of computing.
    \item The lessons of program performance generalize to other computing resources.
    \item Speed is fun!
  \end{itemize}
\end{note}
\section{The problem of sorting}
\begin{problem}[(The problem of sorting)]
\begin{itemize}
  \item Input:sequence $<a_{1},a_{2},\cdots,a_{n}>$ of numbers.
  \item Output: permutation $<a_{1}^{\prime},a_2^{\prime},\cdots,a_{n}^{\prime}>$ such that $a_{1}^{\prime} \leq a_{2}^{\prime} \leq \cdots \leq a_{n}^{\prime}$
\end{itemize}
\end{problem}
\section{Insertion Sort}
\begin{lstlisting}{pseudocode}
    Insertion-Sort(A,n)
      for j <- 2 to n
        do key <- A[j]
          i <- j - 1
          while i > 0 and A[i] > key
            do A[i+1] <- A[i]
            i<- i-1
          A[i+1] = key
\end{lstlisting}

%todo pic-Insertion-Sort

\section{Running time}

\begin{itemize}
  \item The running time depends on the input:an already sorted sequence is easier to sort.
  \item Parameterize the running time by the size of the input,since short sequences are easier to sort than long ones.
  \item Generally,we seek upper bounds on the running time,because everybody likes a guarantee.
\end{itemize}

\subsection{Kinds of Analysis}



\begin{definition}[Worst-Case(usually)]
  $T(n)=$ maximum time of algorithm on any input of size $n$.
\end{definition}
  
\begin{definition}[Average-Case(Sometimes)]
  \begin{itemize}
    \item $T(n)=$ expected time of algorithm over all inputs of size $n$.
    \item Need assumption of statistical distribution of inputs.
  \end{itemize}
  
\end{definition}
\begin{definition}[Best-case: (bogus)]
  Cheat with a slow algorithm that works fast on some input.
\end{definition}
\begin{note}
  What is insertion sort’s worst-case time?
\end{note}
It depends on the speed of our computer:
\begin{itemize}
  \item relative speed (on the same machine),
  \item absolute speed (on different machines).
\end{itemize}

\begin{note}
  BIG IDEA:
\end{note}
\begin{enumerate}
  \item Ignore machine-dependent constants.
  \item look at the growth of $T(n)$ as $n\to \infty$
  \item "Asymptotic Analysis"
\end{enumerate}
\subsection{$\Theta$-Notation}
\begin{definition}[$\Theta$-Notation]
  $\Theta(g(n))=\left\{f(n):\right.$ there exist positive constants $c_{1}, c_{2}$, and $n_{0}$ such that $0 \leq c_{1} g(n) \leq f(n) \leq c_{2} g(n)$ for all $\left.n \geq n_{0}\right\}$
\end{definition}
\begin{note}
  Engineering:Drop low-order terms; Ignore leading constants.
\end{note}
\begin{example}
  $$
3 n^{3}+90 n^{2}-5 n+6046=\Theta\left(n^{3}\right)
$$
\end{example}
\subsection{Asymptotic performance}
\begin{note}
  When $n$ gets large enough, a $\Theta\left(n^{2}\right)$ algorithm always beats a $\Theta\left(n^{3}\right)$ algorithm.
\end{note}

%todo Pic-Asym-Performance

\begin{note}
  \begin{itemize}
    \item We shouldn't ignore asymptotically slower algorithms, however.
    \item Real-world design situations often call for a careful balancing of engineering objectives. 
    \item Asymptotic analysis is a useful tool to help to structure our thinking.
  \end{itemize}
\end{note}
\subsection{Insertion sort analysis}
\subsubsection*{Worst case}
Input reverse sorted
$$
T(n)=\sum_{j=2}^{n} \Theta(j)=\Theta\left(n^{2}\right)
$$
\subsubsection*{Average case}
All permutations equally likely.
$$
T(n)=\sum_{j=2}^{n} \Theta(j / 2)=\Theta\left(n^{2}\right)
$$

\begin{note}
  Is insertion sort a fast sorting algorithm?
  \begin{itemize}
    \item Moderately so, for small $n$
    \item Not at all, for large $n$
  \end{itemize}
\end{note}

\section{Merge Sort}

\begin{lstlisting}[mathescape=true]
  Merge-Sort A[1..n]
    1. If n = 1,done
    2. Recurisively sort $A[1 \ldots\lceil n / 2\rceil]$ and $A[\lceil n / 2\rceil+1 \ldots n]$
    3. Merge the 2 sorted lists.
\end{lstlisting}
\begin{note}
  Key subroutine: MERGE
\end{note}  
\subsection{Analyzing Merge Sort}
Time $=\Theta(n)$ to merge a total of $n$ elements (linear time).
$$
\begin{array}{l|l}
T(n) & \text { MERGE-SORT } A[1 \ldots n] \\
\Theta(1) & \text { 1. If } n=1, \text { done. } \\
2 T(n / 2) & \text { 2. Recursively sort } A[1 \ldots\lceil n / 2\rceil] \\
& \quad \text { and } A[\lceil n / 2\rceil+1 \ldots n] . \\
\Theta(n) & \text { 3. "Merge" the } 2 \text { sorted lists }
\end{array}
$$
\begin{note}
  \textbf{Sloppiness}: $2T(n/2)$ should be $T(\lceil n / 2\rceil)+T(\lfloor n / 2\rfloor)$, but it turns out not to matter asymptotically.
\end{note}
\subsection{Recurrence for merge sort}
$$
T(n)=\left\{\begin{array}{l}
\Theta(1) \text { if } n=1 \\
2 T(n / 2)+\Theta(n) \text { if } n>1
\end{array}\right.
$$
\begin{note}
  \begin{itemize}
    \item We shall usually omit stating the base case when $T(n)=\Theta(1)$ for sufficiently small $n$, but only when it has no effect on the asymptotic solution to the recurrence. 
    \item CLRS and Lecture 2 provide several ways to find a good upper bound on $T(n)$.
  \end{itemize}
\end{note}
\subsection{Recursion tree}
\begin{example}
  Solve $T(n)=2 T(n / 2)+c n$, where $c>0$ is constant.
\end{example}
\subsection{Conclusions}
\begin{itemize}
  \item $\Theta(n \lg n)$ grows more slowly than $\Theta\left(n^{2}\right)$
  \item Therefore, merge sort asymptotically beats insertion sort in the worst case.
  \item In practice, merge sort beats insertion sort for $n > 30$ or so.            
\end{itemize}

\chapter{Asymptotic Notation \& Recurrences}
\begin{introduction}
  \item $O-, \Omega-$, and $\Theta-$ notation
  \item Substitution method
  \item Iterating the recurrence
  \item Recursion tree
  \item Master method
\end{introduction}
\section{Asymptotic notation}
\subsection{$O-$notation (upper bounds)}
\begin{definition}[$O-$notation (upper bounds)]
  We write $f(n)=O(g(n))$ if there exist constants $c>0, n_{0}>0$ such that $0 \leq f(n) \leq \operatorname{cg}(n)$ for all $n \geq n_{0}$
\end{definition}
\begin{example}
  $$
  2 n^{2}=O\left(n^{3}\right) \quad\left(c=1, n_{0}=2\right)
  $$
\end{example}
\begin{note}
  Notice that in this equation, $2n^2$ are functions not values and the eqal sign is just "one-way" equality.
  \\Actually, it can be denoted more precisely: 
  $$
  2 n^{2} \in O\left(n^{3}\right)
  $$
\end{note}
\begin{note}
  Convention: A set in a formula representsan anonymous function in the set.
\end{note}
\begin{example}
  $$
  n^{2}+O(n)=O\left(n^{2}\right)
  $$
  means for any $f(n) \in O(n)$,$n^2 + f(n) = h(n)$ for some $h(n) \in O(n^2)$
\end{example}
\subsection{$\Omega-$ notation(lower bounds)}
O-notation is an upper-bound notation. It makes no sense to say $f(n)$ is at least $O(n^2)$.

\begin{definition}[$\Omega-$ notation(lower bounds)]
  $\Omega(g(n))=\{f(n):$ there exist constants $c>0, n_{0}>0$ such that $0 \leq \operatorname{cg}(n) \leq f(n)$ for all $\left.n \geq n_{0}\right\}$
\end{definition}

\begin{example}
  $$
  \sqrt{n}=\Omega(\lg n) \quad\left(c=1, n_{0}=16\right)
  $$
\end{example}
\subsection{$\Theta-$notation(tight bounds)}
\begin{definition}[$\Theta-$notation(tight bounds)]
  $$
\Theta(g(n))=\mathrm{O}(g(n)) \cap \Omega(g(n))
$$
\end{definition}
\begin{example}
  $$
  \frac{1}{2} n^{2}-2 n=\Theta\left(n^{2}\right)
  $$
\end{example}
\subsection{$o-$notation and $\omega$-notation}
$O$-notation and $\Omega$-notation are like $\leq$ and $\geq$. $o$-notation and $\omega$-notation are like $<$ and $>$.
\begin{definition}[$o-$notation]
  $o(g(n))=\{f(n):$ for any constant $c>0$ ,there is a constant $n_{0}>0$ such that $0 \leq f(n)<c g(n)$ for all $\left.n \geq n_{0}\right\}$ 
\end{definition}
\begin{example}
  $$
2 n^{2}=o\left(n^{3}\right) \quad\left(n_{0}=2 / c\right)
$$
\end{example}
\begin{definition}[$\omega$-notation]
  $o(g(n))=\{f(n):$ for any constant $c>0$ , there is a constant $n_{0}>0$
  Such that $0 \leq f(n)<\operatorname{cg}(n)$
  for all $\left.n \geq n_{0}\right\}$
\end{definition}
\begin{example}
  $$
  \sqrt{n}=\omega(\lg n) \quad\left(n_{0}=1+1 / c\right)
  $$
\end{example}
\section{Solving recurrences}

The analysis of merge sort from Lecture 1 required us to solve a recurrence.Recurrences are like solving integrals, differential equations, etc. Learn a few tricks.

\subsection{Substitution method}
\begin{definition}[Substitution method]
  The most general method:
  \begin{enumerate}
    \item Guess the form of the solution.
    \item Verify by induction.
    \item Solve for constants.
  \end{enumerate} 
\end{definition}

\begin{example}
  $$
T(n)=4 T(n / 2)+n
$$
\end{example}
\begin{itemize}
  \item Assume that $T(1)=\Theta(1)$ .
  \item Guess $O\left(n^{3}\right)$. (Prove $O$ and $\Omega$ separately.)
  \item Assume that $T(k) \leq c k^{3}$ for $k<n$
  \item Prove $T(n) \leq c n^{3}$ by induction. 
\end{itemize}
\begin{solution}
  $$
  \begin{aligned}
  T(n) &=4 T(n / 2)+n \\
  & \leq 4 c(n / 2)^{3}+n \\
  &=(c / 2) n^{3}+n \\
  &=c n^{3}-\left((c / 2) n^{3}-n\right) \leftarrow \text { desired - residual }\\
  & \leq c n^{3} \longleftarrow \text { desired }
  \end{aligned}
  $$

  whenever $(c / 2) n^{3}-n \geq 0$ , for example,if $c \geq 2$ and $n \geq 1$
\end{solution} 
\begin{note}
  We must also handle the initial conditions, that is, ground the induction with base cases.
\end{note}
\begin{enumerate}
  \item \textbf{Base}: $T(n)=\Theta(1)$ for all $n<n_{0}$, where $n_{0}$ is a suitable constant.
  \item For $1 \leq n<n_{0}$, we have  $" \Theta(1) " \leq c n^{3}$, if we pick $c$ big enough.
\end{enumerate}

\begin{center}
\textbf{This bound is not tight!}
\end{center}
\begin{note}
  A tighter upper bound? We shall prove that $T(n)=O\left(n^{2}\right)$
\end{note}
Assume that $T(k) \leq c k^{2}$ for $k<n$ :
$$
\begin{aligned}
T(n) &=4 T(n / 2)+n \\
& \leq 4 c(n / 2)^{2}+n \\
&=c n^{2}+n \\
&=O\left(n^{2}\right)
\end{aligned}
$$
\begin{center}
  \textbf{Wrong! We must prove the I.H.($T(k) \leq ck^2$)}
\end{center}
$$
=c n^{2}-(-n)
$$
for no choice of $c>0$. Lose!
\begin{note}
  \text{IDEA:} Strengthen the inductive hypothesis.(Subtract a low-order term.)
  $$
\text { Inductive hypothesis: } T(k) \leq c_{1} k^{2}-c_{2} k \text { for } k<n \text {. }
$$
\end{note}
\begin{solution}
  $$
  \begin{aligned}
  T(n) &=4 T(n / 2)+n \\
  &=4\left(c_{1}(n / 2)^{2}-c_{2}(n / 2)\right)+n \\
  &=c_{1} n^{2}-2 c_{2} n+n \\
  &=c_{1} n^{2}-c_{2} n-\left(c_{2} n-n\right) \\
  & \leq c_{1} n^{2}-c_{2} n \text { if } c_{2} \geq 1 .
  \end{aligned}
  $$
Pick $c_1$ big enough to handle the initial conditions.
\end{solution}
\subsection{Recursion-tree method}
\begin{itemize}
  \item A recursion tree models the costs(time)ofa recursive execution of an algorithm.

  \item The recursion-tree method can be unreliable, just like any method that uses ellipses (...) 
  
  \item The recursion-tree method promotes intuition, however.
  \item The recursion tree method is good for generating guesses for the substitution method.
\end{itemize}
\begin{example}
  Solve 
  $$
  T(n)=T(n / 4)+T(n / 2)+n^{2}
  $$
  %todo recursion-tree-pic
\end{example}
\subsection{The master method}
The master method applies to recurrences of the form
$$
T(n)=a T(n / b)+f(n)
$$

where $a \geq 1, b>1$ and $f$ is asymptotically positive.
\begin{theorem}[Three common cases]
  Compare $f(n)$ with $n^{\log _{b} a}$ :
  \begin{enumerate}
    \item $f(n)=O\left(n^{\log _{b} a-\varepsilon}\right)$ for some constant $\varepsilon>0$
    \begin{itemize}
      \item $f(n)$ grows polynomially slower than $n^{\log _{b} a}$ (by an $n^{\varepsilon}$ factor).
    \end{itemize}
    \textbf{Solution:}$$
    T(n)=\Theta\left(n^{\log _{b} a}\right)
    $$
    \item $f(n)=\Theta\left(n^{\log _{b} a} \lg ^{k} n\right)$ for some constant $k \geq 0$
    \begin{itemize}
      \item $f(n)$ and $n^{\log _{b} a}$ grow at similar rates.
    \end{itemize}
    \textbf{Solution:}
    $$
    T(n)=\Theta\left(n^{\log _{b} a} \lg ^{k+1} n\right)
    $$
    \item $f(n)=\Omega\left(n^{\log _{b} a+\varepsilon}\right)$ for some constant $\varepsilon>0$
    \begin{itemize}
      \item $f(n)$ grows polynomially faster than $n^{\log _{b} a}$ (by an $n^{\varepsilon}$ factor),and $f(n)$ satisfies the regularity condition that $a f(n / b) \leq c f(n)$ for some constant $c<1$
    \end{itemize}
    \textbf{Solution:}
    $$
    T(n)=\Theta(f(n))
    $$ 
  \end{enumerate}
\end{theorem}
\begin{example}
  $$
T(n)=4 T(n / 2)+n^{3}
$$
\end{example}
\begin{solution}
  $$a=4, b=2 \Rightarrow n^{\log _{b} a}=n^{2} ; f(n)=n^{3}$$

  \textbf{CASE 3}: $f(n)=\Omega\left(n^{2+\varepsilon}\right)$ for $\varepsilon=1$ and $4(n / 2)^{3} \leq c n^{3}$ (reg. cond.) for $c=1 / 2$.
  $$
\therefore T(n)=\Theta\left(n^{3}\right)
$$
\end{solution}
\begin{example}
  $$
  T(n)=4 T(n / 2)+n^{2} / \lg n
  $$
\end{example}
\begin{solution}
  $$a=4, b=2 \Rightarrow n^{\log _{b} a}=n^{2} ; f(n)=n^{2} / \lg n$$

  \textbf{Master method does not apply.} In particular, for every constant $\varepsilon>0$, we have $n^{\varepsilon}=\omega(\lg n)$.

\end{solution}

\subsubsection{Idea of master theorem}
%todo recursion-tree-master-theorem'

\chapter{Divide and Conquer}

\begin{introduction}
    \item Binary search
    \item Powering a number
    \item Fibonacci numbers
    \item Matrix multiplication
    \item Strassen’s algorithm
    \item VLSI tree layout
\end{introduction}

\section{The divide-and-conquer design paradigm}
\begin{enumerate}
    \item Divide the problem (instance) into subproblems.
    \item Conquer the subproblems by solving them recursively.
    \item Combine subproblem solutions.
\end{enumerate}
\begin{note}
Merge Sort
\begin{enumerate}
    \item Divide: Trivial.
    \item Conguer:Recursively sort 2 subarrays.
    \item Combine:Linear-time merge.
\end{enumerate}
$$
T(n)=2 T(n / 2)+\Theta(n)
$$

Using Master theorem :

Merge sort: $a=2, b=2 \Rightarrow n^{\log _{b} a}=n^{\log _{2} 2}=n$ $\Rightarrow$ CASE 2 $(k=0) \Rightarrow T(n)=\Theta(n \lg n)$.
\end{note}
\section{Binary search}
Find an element in a sorted array:
\begin{enumerate}
    \item Divide: Check middle element
    \item Conquer: Recursively search 1 subarray.
    \item Combine: Trivial.
\end{enumerate}
$$
T(n)=1 T(n / 2)+\Theta(1)
$$
$$
\begin{gathered}
n^{\log _{b} a}=n^{\log _{2} 1}=n^{0}=1 \Rightarrow \text { CASE } 2(k=0) \\
\Rightarrow T(n)=\Theta(\lg n)
\end{gathered}
$$
\section{Powering a number}

\begin{problem}
    Compute $a^{n}$, where $n \in \mathbb{N}$.
\end{problem}
\begin{solution}
    \paragraph*{Naive algorithm:}
        $\Theta(n)$
    \paragraph*{Divide-and-conquer algorithm:}
    $$
    a^{n}= \begin{cases}a^{n / 2} \cdot a^{n / 2} & \text { if } n \text { is even } \\ a^{(n-1) / 2} \cdot a^{(n-1) / 2} \cdot a & \text { if } n \text { is odd }\end{cases}
    $$
    $$
    T(n)=T(n / 2)+\Theta(1) \Rightarrow T(n)=\Theta(\lg n)
    $$
\end{solution}
\section{Fibonacci numbers}
\paragraph*{Recursive definition:}
$$
F_{n}= \begin{cases}0 & \text { if } n=0 \\ 1 & \text { if } n=1 \\ F_{n-1}+F_{n-2} & \text { if } n \geq 2\end{cases}
$$
\begin{note}
    \textbf{Naive recursive algorithm:} $\Omega\left(\phi^{n}\right)$ (exponential time), where $\phi=(1+\sqrt{5}) / 2$ is the golden ratio.
\end{note}
\begin{note}
    \begin{itemize}
        \item \textbf{Bottom-up:}
        \begin{itemize}
            \item Compute $F_{0}, F_{1}, F_{2}, \ldots, F_{n}$ in order, forming each number by summing the two previous. 
            \item Running time: $\Theta(n)$.
        \end{itemize}
        \item \textbf{Naive recursive squaring:}
        
        $F_{n}=\phi^{n} / \sqrt{5}$ rounded to the nearest integer.
        \begin{itemize}
            \item Recursive squaring: $\Theta(\lg n)$ time.
            \item This method is unreliable, since floating-point arithmetic is prone to round-off errors.
        \end{itemize}
    \end{itemize}
\end{note}
\begin{theorem}[Recursive squaring]
    $$
\left[\begin{array}{cc}
F_{n+1} & F_{n} \\
F_{n} & F_{n-1}
\end{array}\right]=\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right]^{n}
$$
\end{theorem}
\begin{note}
    Algorithm: Recursive squaring.
    $$
\text { Time }=\Theta(\lg n)
$$
\end{note}
\begin{proof}
    \begin{itemize}
        \item Base (n = 1):
        $$
\left[\begin{array}{ll}
F_{2} & F_{1} \\
F_{1} & F_{0}
\end{array}\right]=\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right]^{1}
$$
        \item Inductive step ($n\geq2$):
        $$
\begin{aligned}
{\left[\begin{array}{cc}
F_{n+1} & F_{n} \\
F_{n} & F_{n-1}
\end{array}\right] } &=\left[\begin{array}{cc}
F_{n} & F_{n-1} \\
F_{n-1} & F_{n-2}
\end{array}\right] \cdot\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right] \\
&=\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right]^{n-1} \cdot\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right] \\
&=\left[\begin{array}{ll}
1 & 1 \\
1 & 0
\end{array}\right]^{n}
\end{aligned}
$$
    \end{itemize}
\end{proof}
\section{Matrix multiplication}
\begin{definition}[Matrix multiplication]
    $$
\left.\begin{array}{ll}
\text { Input: } & A=\left[a_{i j}\right], B=\left[b_{i j}\right] . \\
\text { Output: } & C=\left[c_{i j}\right]=A \cdot B .
\end{array}\right\} i, j=1,2, \ldots, n .
$$
$$
\left[\begin{array}{cccc}
c_{11} & c_{12} & \cdots & c_{1 n} \\
c_{21} & c_{22} & \cdots & c_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
c_{n 1} & c_{n 2} & \cdots & c_{n n}
\end{array}\right]=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right] \cdot\left[\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 n} \\
b_{21} & b_{22} & \cdots & b_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n 1} & b_{n 2} & \cdots & b_{n n}
\end{array}\right]
$$
$$
c_{i j}=\sum_{k=1}^{n} a_{i k} \cdot b_{k j}
$$
\end{definition}
\subsection{Standard algorithm}

\begin{lstlisting}[mathescape=true]
    for i <- 1 to n
        do for j <- 1 to n
            do $c_{ij} <- 0$
                for k <- 1 to n
                    do $c_{ij}$ <- $c_{ij}$ + $a_{ik}  \cdot b_{kj}$
\end{lstlisting}

Running time $=\Theta\left(n^{3}\right)$
\subsection{Divide-and-conquer algorithm}
\begin{note}
    $n \times n$ matrix $=2 \times 2$ matrix of $(n / 2) \times(n / 2)$ submatrices:
    $$
\left[\begin{array}{c:c}
r & s \\
\hdashline t & u
\end{array}\right]=\left[\begin{array}{l:l}
a & b \\
\hdashline c & d
\end{array}\right] \cdot\left[\begin{array}{c:c}
e & f \\
\hdashline g & h
\end{array}\right]
$$
$$
C=A \cdot B
$$
$$
\left.\begin{array}{l}
r=a e+b g \\
s=a f+b h \\
t=c e+d g \\
u=c f+d h
\end{array}\right\} \quad 8 \text { mults of }(n / 2) \times(n / 2) \text { submatrices }
$$
\end{note}
$$
T(n)=8 T(n / 2)+\Theta\left(n^{2}\right)
$$
$$
n^{\log _{b} a}=n^{\log _{2} 8}=n^{3} \Rightarrow \text { CASE } 1 \Rightarrow T(n)=\Theta\left(n^{3}\right)
$$
\begin{center}
    \textbf{No better than the ordinary algorithm.}
\end{center}
\section{Strassen’s Algorithm}
\subsection{Strassen’s idea}
Multiply $2 \times 2$ matrices with only 7 recursive mults.
$$
\begin{array}{ll}
P_{1}=a \cdot(f-h) & r=P_{5}+P_{4}-P_{2}+P_{6} \\
P_{2}=(a+b) \cdot h & s=P_{1}+P_{2} \\
P_{3}=(c+d) \cdot e & t=P_{3}+P_{4} \\
P_{4}=d \cdot(g-e) & u=P_{5}+P_{1}-P_{3}-P_{7} \\
P_{5}=(a+d) \cdot(e+h) & \\
P_{6}=(b-d) \cdot(g+h) & \\
P_{7}=(a-c) \cdot(e+f) &
\end{array}
$$
\begin{note}
    7 mults, 18 adds/subs. \textbf{No reliance on commutativity of mult!}
\end{note}
\subsection{Strassen’s algorithm}
\begin{enumerate}
    \item Divide: Partition $A$ and $B$ into $(n / 2) \times(n / 2)$ submatrices. Form terms to be multiplied using $+$ and $-$.
    \item Conquer: Perform 7 multiplications of $(n / 2) \times(n / 2)$ submatrices recursively.
    \item Combine: Form $C$ using $+$ and $-$ on $(n / 2) \times(n / 2)$ submatrices.
\end{enumerate}
$$
T(n)=7 T(n / 2)+\Theta\left(n^{2}\right)
$$
$$
n^{\log _{b} a}=n^{\log _{2} 7} \approx n^{2.81} \Rightarrow \text { CASE } 1 \Rightarrow T(n)=\Theta\left(n^{\lg 7}\right)
$$
\begin{note}
    The number $2.81$ may not seem much smaller than 3 , but because the difference is in the exponent, the impact on running time is significant. In fact, Strassen's algorithm beats the ordinary algorithm on today's machines for $n \geq 32$ or so.
\end{note}
\begin{note}
    Best to date (of theoretical interest only): $\Theta\left(n^{2.376 \cdots}\right)$
\end{note}
\section{VLSI layout}
\begin{problem}
    Embed a complete binary tree with n leaves in a grid using minimal area.
\end{problem}
%todo: 
\section{Conclusion}

\begin{itemize}
    \item Divide and conquer is just one of several powerful techniques for algorithm design.
    \item Divide-and-conquer algorithms can be analyzed using recurrences and the master method (so practice this math).
    \item The divide-and-conquer strategy often leads to efficient algorithms.
\end{itemize}


\chapter{Quicksort}

\begin{introduction}
\item Divide and conquer
\item Partitioning
\item Worst-case analysis
\item Intuition
\item Randomized quicksort
\item Analysis
\end{introduction}
\section{Quicksort}

\begin{note}
    \begin{itemize}
        \item Proposed by C.A.R.Hoare in 1962.
    
        \item Divide-and-conquer algorithm.
        
        \item Sorts"in place"(like insertion sort,but not like merge sort).
        
        \item Very practical (with tuning).
    \end{itemize}
\end{note}
\subsection{Divide and conquer}
Quicksort an $n-$element array:
\begin{enumerate}
    \item \textbf{Divide:} Partition the array into two subarrays around a \textbf{pivot} $x$ such that elements in lower subarray $\leq x \leq$ elements in upper subarray.
    %todo pic
    \item \textbf{Conquer:}Recursively sort the two subarrays.
    \item \textbf{Combine:}Trivial.
\end{enumerate}
\begin{center}
    \textbf{Key:} \textit{Linear-time Partitioning subroutine.}
\end{center}
\subsection{Partitioning subroutine}\

\begin{lstlisting}[mathescape=true]
    PARTITION(A,p,q)  //A[p..q]
        x <- A[p]     //pivot = A[p]
        i <- p
        for J <- p + 1 to q
            do if A[j] $\leq$ x
                then i <- i + 1
                    exchange A[i] <-> A[j]
    exchange A[p] <-> A[i]
    return i
\end{lstlisting}
\begin{note}
    Running time $=O(n)$ for $n$ elements.
\end{note}
\begin{example}
    Example of partitioning
\end{example}
%todo
\subsection{Pseudocode for quicksort}
\begin{lstlisting}[mathescape=true]
    QUICKSORT(A,p,r)
        if p < r
            then q <- PARTITION(A,p,r)
                QUICKSORT(A,p,q-1)
                QUICKSORT(A,q+1,r)

        Initial call: QUICKSORT(A, 1, n)
\end{lstlisting}  
\section{Analysis of quicksort}
\begin{enumerate}
    \item Assume all input elements are distinct.

    \item In practice,there are better partitioning algorithms for when duplicate input elements may exist.
    
    \item Let $T(n)=$worst-case running time on an array of $n$ elements.
\end{enumerate}
\subsection{Worst-case of quicksort}
\begin{itemize}
    \item Input sorted or reverse sorted.

    \item Partition around min or max element.
    
    \item One side of partition always has no elements.
\end{itemize}
$$
\begin{aligned}
T(n) &=T(0)+T(n-1)+\Theta(n) \\
&=\Theta(1)+T(n-1)+\Theta(n) \\
&=T(n-1)+\Theta(n) \\
&=\Theta\left(n^{2}\right) \quad \text { (arithmetic series) }
\end{aligned}
$$
\subsubsection{Worst-case recursion tree}
%todo 
\subsection{Best-case analysis}
If we're lucky,PARTITION splits the array evenly:
$$
\begin{aligned}
T(n) &=2 T(n / 2)+\Theta(n) \\
&=\Theta(n \lg n) \quad \quad(\text { same as merge sort })
\end{aligned}
$$

What if the split is always $\frac{1}{10}: \frac{9}{10} ?$
$$
T(n)=T\left(\frac{1}{10} n\right)+T\left(\frac{9}{10} n\right)+\Theta(n)
$$
What is the solution to this recurrence?
%todo recurrence
$$
c n \log _{10} n \leq T(n) \leq c n \log _{10 / 9} n+O(n)
$$
\subsection{More intuition}
Suppose we alternate lucky, unlucky, lucky, unlucky, lucky, ....
$$
\begin{array}{ll}
L(n)=2 U(n / 2)+\Theta(n) & \text { lucky } \\
U(n)=L(n-1)+\Theta(n) & \text { unlucky }
\end{array}
$$
Solving:
$$
\begin{aligned}
L(n) &=2(L(n / 2-1)+\Theta(n / 2))+\Theta(n) \\
&=2 L(n / 2-1)+\Theta(n) \\
&=\Theta(n \lg n) \text { Lucky! }
\end{aligned}
$$
How can we make sure we are usually lucky?
\section{Randomized quicksort}
    \begin{note}
        \textbf{IDEA:}Partition around a random element.
        \begin{itemize}
            \item Running time is independent of the input order.
            \item No assumptions need to be made about the input distribution.
            \item No specific input elicits the worst-case behavior.
            \item The worst case is determined only by the output of a random-number generator.
        \end{itemize}
    \end{note}
\section{Randomized quicksort analysis}
Let $T(n)=$ the random variable for the running time of randomized quicksort on an input of size $n$, assuming random numbers are independent.

For $k=0,1, \ldots, n-1$, define the indicator random variable

$$
X_{k}= \begin{cases}1 & \text { if PARTITION generates a } k: n-k-1 \text { split, } \\ 0 & \text { otherwise. }\end{cases}
$$

$E\left[X_{k}\right]=\operatorname{Pr}\left\{X_{k}=1\right\}=1 / n$, since all splits are equally likely, assuming elements are distinct.
$$
T(n)=\left\{\begin{array}{c}
T(0)+T(n-1)+\Theta(n) \text { if } 0: n-1 \text { split } \\
T(1)+T(n-2)+\Theta(n) \text { if } 1: n-2 \text { split } \\
\vdots \\
T(n-1)+T(0)+\Theta(n) \text { if } n-1: 0 \text { split }
\end{array}\right.
$$
$$
=\sum_{k=0}^{n-1} X_{k}(T(k)+T(n-k-1)+\Theta(n))
$$
\subsubsection*{Calculating expectation}
$$
\begin{aligned}
E[T(n)] &=E\left[\sum_{k=0}^{n-1} X_{k}(T(k)+T(n-k-1)+\Theta(n))\right] \\
&=\sum_{k=0}^{n-1} E\left[X_{k}(T(k)+T(n-k-1)+\Theta(n))\right] \\
&=\sum_{k=0}^{n-1} E\left[X_{k}\right] \cdot E[T(k)+T(n-k-1)+\Theta(n)]
\end{aligned}
$$
\begin{note}
    Independence of $X_{k}$ from other random choices.  
\end{note}
$$
\begin{aligned}
&=\frac{1}{n} \sum_{k=0}^{n-1} E[T(k)]+\frac{1}{n} \sum_{k=0}^{n-1} E[T(n-k-1)]+\frac{1}{n} \sum_{k=0}^{n-1} \Theta(n)\\
&=\frac{2}{n} \sum_{k=1}^{n-1} E[T(k)]+\Theta(n) \quad \begin{aligned}
&\text { Summations have } \\
&\text { identical terms. }
\end{aligned}
\end{aligned}
$$
(The $k=0,1$ terms can be absorbed in the $\Theta(n) .)$
\begin{proof}
    $E[T(n)] \leq a n \lg n$ for constant $a>0$.Choose $a$ large enough so that $a n \lg n$ dominates $E[T(n)]$ for sufficiently small $n \geq 2$.
\end{proof}
Use fact:
$$
\sum_{k=2}^{n-1} k \lg k \leq \frac{1}{2} n^{2} \lg n-\frac{1}{8} n^{2} 
$$
$$
\begin{aligned}
E[T(n)] & \leq \frac{2}{n} \sum_{k=2}^{n-1} a k \lg k+\Theta(n) \\
& \leq \frac{2 a}{n}\left(\frac{1}{2} n^{2} \lg n-\frac{1}{8} n^{2}\right)+\Theta(n) \\
&=a n \lg n-\left(\frac{a n}{4}-\Theta(n)\right)
\end{aligned}
$$
Express as \textbf{desired – residual.}

if $a$ is chosen large enough so that an/4 dominates the $\Theta(n)$.
\section{Quicksort in practice}
\begin{itemize}
    \item Quicksort is a great general-purpose sorting algorithm.
    \item Quicksort is typically over twice as fast as merge sort.

    \item Quicksort can benefit substantially from code tuning.
    
    \item Quicksort behaves well even with caching and virtual memory.
\end{itemize}


\chapter{Sorting Lower Bounds}
\begin{introduction}
    \item Decision trees
    \item Counting sort
    \item Radix sort
    \item Appendix: Punched cards
\end{introduction}
\section{How fast can we sort?}
All the sorting algorithms we have seen so far are \textbf{comparison sorts} :only use comparisons to determine the relative order of elements.

\begin{itemize}
    \item \textbf{E.g.} insertion sort, merge sort, quicksort, heapsort.
\end{itemize}

The best worst-case running time that we've seen for comparison sorting is $O(n \lg n)$.

\begin{center}
    \textbf{Is $O(n \lg n)$ the best we can do?}
\end{center}

\textbf{Decision trees} can help us answer this question.

\section{Decision Tree}
\subsection{Decision-tree Example}
%todo 
\begin{definition}[Decision-tree model]
    A decision tree can model the execution of any comparison sort:
    \begin{itemize}
        \item One tree for each input size $n$.
        \item View the algorithm as splitting whenever it compares two elements.
        \item The tree contains the comparisons along all possible instruction traces.
        \item The running time of the algorithm $=$ the length of the path taken.
        \item Worst-case running time $=$ height of tree.
    \end{itemize}
\end{definition}
\subsection{Lower bound for decisiontree sorting}
\begin{theorem}
    Any decision tree that can sort $n$ elements must have height $\Omega(n \lg n)$.
\end{theorem}
\begin{proof}
    The tree must contain $\geq n$ ! leaves, since there are $n$ ! possible permutations. A height- $h$ binary tree has $\leq 2^{h}$ leaves. Thus, $n ! \leq 2^{h}$.
    $$
\begin{aligned}
h & \geq \lg (n !) & &(\lg \text { is mono. increasing) }\\
& \geq \lg \left((n / e)^{n}\right) & &(\text { Stirling's formula) }\\
&=n \lg n-n \lg e & \\
&=\Omega(n \lg n)
\end{aligned}
$$
\end{proof}
\begin{corollary}
    Heapsort and merge sort are \textbf{asymptotically optimal comparison sorting algorithms}.
\end{corollary}
\section{Sorting in linear time}
\subsection{Counting sort}
No comparisons between elements.
\begin{itemize}
    \item Input: $A[1 \ldots n]$, where $A[j] \in\{1,2, \ldots, k\}$.
    \item Output: $B[1$.. $n]$, sorted.
    \item Auxiliary storage: $C[1 \ldots k]$.
\end{itemize}
\begin{lstlisting}
    for i <- 1 to k
        do C[i] <- 0
    for j <- 1 to n
        do C[A[j]] <- C[A[]] + 1 .  //C[i] = |{key = i}|
    for i <- 2 to k
        do C[i] <- C[i] +C[i-1]
    for j <- n downto 1
        do B[C[A[j]]] <- A[j]
           C[A[j]] <- C[A[j]] - 1 
\end{lstlisting}
\begin{note}
\textbf{Analysis}
$$
\begin{aligned}
&\Theta(k)\left\{\begin{array}{r}
\text { for } i \leftarrow 1 \text { to } k \\
\text { do } C[i] \leftarrow 0
\end{array}\right.\\
&\Theta(n)\left\{\begin{array}{c}
\text { for } j \leftarrow 1 \text { to } n \\
\text { do } C[A[j]] \leftarrow C[A[j]]+1
\end{array}\right.\\
&\Theta(k)\left\{\begin{array}{c}
\text { for } i \leftarrow 2 \text { to } k \\
\text { do } C[i] \leftarrow C[i]+C[i-1]
\end{array}\right.\\
&\Theta(n)\left\{\begin{array}{c}
\text { for } j \leftarrow n \text { downto } 1 \\
\text { do } B[C[A[j]]] \leftarrow \mathrm{A}[j] \\
C[A[j]] \leftarrow C[A[j]]-1
\end{array}\right.
\end{aligned}
$$
\end{note}
\subsubsection{Running time}
If $k=O(n)$, then counting sort takes $\Theta(n)$ time.
\begin{itemize}
    \item But, sorting takes $\Omega(n \lg n)$ time! 
    \item Where's the fallacy?
\end{itemize}
\begin{solution}
    \begin{itemize}
        \item Comparison sorting takes $\Omega(n \lg n)$ time.
        \item Counting sort is not a comparison sort.
        \item In fact, not a single comparison between elements occurs!
    \end{itemize}
\end{solution}
\begin{definition}[Stable sorting]
    Counting sort is a \textbf{stable sort}: it preserves the input order among equal elements.
\end{definition}
\begin{exercise}
    What other sorts have this property?
\end{exercise}
\subsection{Radix sort}
\begin{note}
    \begin{itemize}
        \item Origin: Herman Hollerith's card-sorting machine for the 1890 U.S. Census. 
        \item Digit-by-digit sort.
        \item Hollerith's original (bad) idea: sort on most-significant digit first.
        \item Good idea: Sort on least-significant digit first with auxiliary stable sort.
    \end{itemize}
\end{note}
\subsubsection{Operation of radix sort}
%todo
\subsubsection{Correctness of radix sort}
Induction on digit position 
\begin{itemize}
    \item Assume that the numbers are sorted by their low-order $t-1$ digits.
    \item Sort on digit $t$
    \begin{itemize}
        \item Two numbers that differ in digit $t$ are correctly sorted.
        \item Two numbers equal in digit $t$ are put in the same order as the input $\Rightarrow$ correct order.
    \end{itemize}
    
\end{itemize}

\subsubsection{Analysis of radix sort}

\begin{itemize}
    \item Assume counting sort is the auxiliary stable sort.
    \item Sort $n$ computer words of $b$ bits each.
    \item Each word can be viewed as having $b / r$ base- $2^{r}$ digits.
\end{itemize}

\begin{example}
    32-bit word
%todo Pic
    $r=8 \Rightarrow b / r=4$ passes of counting sort on
base- $2^{8}$ digits; or $r=16 \Rightarrow b / r=2$ passes of counting sort on base-216 digits.

\begin{center}
    \textbf{How many passes should we make?}
\end{center}
\end{example}
\begin{note}
    \textbf{Recall:} Counting sort takes $\Theta(n+k)$ time to sort $n$ numbers in the range from 0 to $k-1$.
\end{note}
If each $b$-bit word is broken into $r$-bit pieces, each pass of counting sort takes $\Theta\left(n+2^{r}\right)$ time. Since there are $b / r$ passes, we have
    $$
T(n, b)=\Theta\left(\frac{b}{r}\left(n+2^{r}\right)\right) .
$$
Choose $r$ to minimize $T(n, b):$
\begin{itemize}
    \item Increasing $r$ means fewer passes, but as $r \gg \lg n$, the time grows exponentially.Minimize $T(n, b)$ by differentiating and setting to 0 .Or, just observe that we don't want $2^{r} \gg n$, and there's no harm asymptotically in choosing $r$ as large as possible subject to this constraint.Choosing $r=\lg n$ implies $T(n, b)=\Theta(b n / \lg n)$.
    \item For numbers in the range from 0 to $n^{d}-1$, we have $b=d \lg n \Rightarrow$ radix sort runs in $\Theta(d n)$ time.
\end{itemize}
\section{Colclusions}
In practice, radix sort is fast for large inputs, as well as simple to code and maintain.
\begin{example}
    \begin{enumerate}
        \item At most 3 passes when sorting $\geq 2000$ numbers. 
        \item Merge sort and quicksort do at least $\lceil\lg 2000\rceil=$ 11 passes.
    \end{enumerate}
\end{example}

\begin{note}
    \textbf{Downside:} Unlike quicksort, radix sort displays little locality of reference, and thus a well-tuned quicksort fares better on modern processors, which feature steep memory hierarchies.
\end{note}
\end{document}