\documentclass[11pt]{elegantbook}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}


\title{CLRS Notes}
\subtitle{MIT 6.046J}

\author{Haopeng Li}
%\institute{Elegant\LaTeX{} Program}
\date{\today}
%\bioinfo{Bio}{Information}

%\extrainfo{Victory won\rq t come to us unless we go to it. }

\logo{logo-blue.png}
\cover{cover.jpg}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}

\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Analysis of Algorithms}
\begin{introduction}
  \item Insertion sort
  \item Asymptotic analysis
  \item Merge sort
  \item Recurrences
\end{introduction}
\begin{definition}[Algorithms]
  The theoretical study of computer-program performance and resource usage.
\end{definition}

\begin{note}
  Why study algorithms and performance?
  \begin{itemize}
    \item Algorithms help us to understand scalability.
    \item Performance often draws the line between what is feasible and what is impossible.
    \item Algorithmic mathematics provides a language for talking about program behavior.
    \item Performance is the currency of computing.
    \item The lessons of program performance generalize to other computing resources.
    \item Speed is fun!
  \end{itemize}
\end{note}
\section{The problem of sorting}
\begin{problem}[(The problem of sorting)]
\begin{itemize}
  \item Input:sequence $<a_{1},a_{2},\cdots,a_{n}>$ of numbers.
  \item Output: permutation $<a_{1}^{\prime},a_2^{\prime},\cdots,a_{n}^{\prime}>$ such that $a_{1}^{\prime} \leq a_{2}^{\prime} \leq \cdots \leq a_{n}^{\prime}$
\end{itemize}
\end{problem}
\section{Insertion Sort}
\begin{lstlisting}{pseudocode}
    Insertion-Sort(A,n)
      for j <- 2 to n
        do key <- A[j]
          i <- j - 1
          while i > 0 and A[i] > key
            do A[i+1] <- A[i]
            i<- i-1
          A[i+1] = key
\end{lstlisting}

%todo pic-Insertion-Sort

\section{Running time}

\begin{itemize}
  \item The running time depends on the input:an already sorted sequence is easier to sort.
  \item Parameterize the running time by the size of the input,since short sequences are easier to sort than long ones.
  \item Generally,we seek upper bounds on the running time,because everybody likes a guarantee.
\end{itemize}

\subsection{Kinds of Analysis}



\begin{definition}[Worst-Case(usually)]
  $T(n)=$ maximum time of algorithm on any input of size $n$.
\end{definition}
  
\begin{definition}[Average-Case(Sometimes)]
  \begin{itemize}
    \item $T(n)=$ expected time of algorithm over all inputs of size $n$.
    \item Need assumption of statistical distribution of inputs.
  \end{itemize}
  
\end{definition}
\begin{definition}[Best-case: (bogus)]
  Cheat with a slow algorithm that works fast on some input.
\end{definition}
\begin{note}
  What is insertion sortâ€™s worst-case time?
\end{note}
It depends on the speed of our computer:
\begin{itemize}
  \item relative speed (on the same machine),
  \item absolute speed (on different machines).
\end{itemize}

\begin{note}
  BIG IDEA:
\end{note}
\begin{enumerate}
  \item Ignore machine-dependent constants.
  \item look at the growth of $T(n)$ as $n\to \infty$
  \item "Asymptotic Analysis"
\end{enumerate}
\subsection{$\Theta$-Notation}
\begin{definition}[$\Theta$-Notation]
  $\Theta(g(n))=\left\{f(n):\right.$ there exist positive constants $c_{1}, c_{2}$, and $n_{0}$ such that $0 \leq c_{1} g(n) \leq f(n) \leq c_{2} g(n)$ for all $\left.n \geq n_{0}\right\}$
\end{definition}
\begin{note}
  Engineering:Drop low-order terms; Ignore leading constants.
\end{note}
\begin{example}
  $$
3 n^{3}+90 n^{2}-5 n+6046=\Theta\left(n^{3}\right)
$$
\end{example}
\subsection{Asymptotic performance}
\begin{note}
  When $n$ gets large enough, a $\Theta\left(n^{2}\right)$ algorithm always beats a $\Theta\left(n^{3}\right)$ algorithm.
\end{note}

%todo Pic-Asym-Performance

\begin{note}
  \begin{itemize}
    \item We shouldn't ignore asymptotically slower algorithms, however.
    \item Real-world design situations often call for a careful balancing of engineering objectives. 
    \item Asymptotic analysis is a useful tool to help to structure our thinking.
  \end{itemize}
\end{note}
\subsection{Insertion sort analysis}
\subsubsection*{Worst case}
Input reverse sorted
$$
T(n)=\sum_{j=2}^{n} \Theta(j)=\Theta\left(n^{2}\right)
$$
\subsubsection*{Average case}
All permutations equally likely.
$$
T(n)=\sum_{j=2}^{n} \Theta(j / 2)=\Theta\left(n^{2}\right)
$$

\begin{note}
  Is insertion sort a fast sorting algorithm?
  \begin{itemize}
    \item Moderately so, for small $n$
    \item Not at all, for large $n$
  \end{itemize}
\end{note}

\section{Merge Sort}

\begin{lstlisting}[mathescape=true]
  Merge-Sort A[1..n]
    1. If n = 1,done
    2. Recurisively sort $A[1 \ldots\lceil n / 2\rceil]$ and $A[\lceil n / 2\rceil+1 \ldots n]$
    3. Merge the 2 sorted lists.
\end{lstlisting}
\begin{note}
  Key subroutine: MERGE
\end{note}  
\subsection{Analyzing Merge Sort}
Time $=\Theta(n)$ to merge a total of $n$ elements (linear time).
$$
\begin{array}{l|l}
T(n) & \text { MERGE-SORT } A[1 \ldots n] \\
\Theta(1) & \text { 1. If } n=1, \text { done. } \\
2 T(n / 2) & \text { 2. Recursively sort } A[1 \ldots\lceil n / 2\rceil] \\
& \quad \text { and } A[\lceil n / 2\rceil+1 \ldots n] . \\
\Theta(n) & \text { 3. "Merge" the } 2 \text { sorted lists }
\end{array}
$$
\begin{note}
  \textbf{Sloppiness}: $2T(n/2)$ should be $T(\lceil n / 2\rceil)+T(\lfloor n / 2\rfloor)$, but it turns out not to matter asymptotically.
\end{note}
\subsection{Recurrence for merge sort}
$$
T(n)=\left\{\begin{array}{l}
\Theta(1) \text { if } n=1 \\
2 T(n / 2)+\Theta(n) \text { if } n>1
\end{array}\right.
$$
\begin{note}
  \begin{itemize}
    \item We shall usually omit stating the base case when $T(n)=\Theta(1)$ for sufficiently small $n$, but only when it has no effect on the asymptotic solution to the recurrence. 
    \item CLRS and Lecture 2 provide several ways to find a good upper bound on $T(n)$.
  \end{itemize}
\end{note}
\subsection{Recursion tree}
\begin{example}
  Solve $T(n)=2 T(n / 2)+c n$, where $c>0$ is constant.
\end{example}
\subsection{Conclusions}
\begin{itemize}
  \item $\Theta(n \lg n)$ grows more slowly than $\Theta\left(n^{2}\right)$
  \item Therefore, merge sort asymptotically beats insertion sort in the worst case.
  \item In practice, merge sort beats insertion sort for $n > 30$ or so.            
\end{itemize}
\chapter{Asymptotic Notation \& Recurrences}
\begin{introduction}
  \item $O-, \Omega-$, and $\Theta-$ notation
  \item Substitution method
  \item Iterating the recurrence
  \item Recursion tree
  \item Master method
\end{introduction}
\section{Asymptotic notation}
\subsection{$O-$notation (upper bounds)}
\begin{definition}[$O-$notation (upper bounds)]
  We write $f(n)=O(g(n))$ if there exist constants $c>0, n_{0}>0$ such that $0 \leq f(n) \leq \operatorname{cg}(n)$ for all $n \geq n_{0}$
\end{definition}
\begin{example}
  $$
  2 n^{2}=O\left(n^{3}\right) \quad\left(c=1, n_{0}=2\right)
  $$
\end{example}
\begin{note}
  Notice that in this equation, $2n^2$ are functions not values and the eqal sign is just "one-way" equality.
  \\Actually, it can be denoted more precisely: 
  $$
  2 n^{2} \in O\left(n^{3}\right)
  $$
\end{note}
\begin{note}
  Convention: A set in a formula representsan anonymous function in the set.
\end{note}
\begin{example}
  $$
  n^{2}+O(n)=O\left(n^{2}\right)
  $$
  means for any $f(n) \in O(n)$,$n^2 + f(n) = h(n)$ for some $h(n) \in O(n^2)$
\end{example}
\subsection{$\Omega-$ notation(lower bounds)}
O-notation is an upper-bound notation. It makes no sense to say $f(n)$ is at least $O(n^2)$.

\begin{definition}[$\Omega-$ notation(lower bounds)]
  $\Omega(g(n))=\{f(n):$ there exist constants $c>0, n_{0}>0$ such that $0 \leq \operatorname{cg}(n) \leq f(n)$ for all $\left.n \geq n_{0}\right\}$
\end{definition}

\begin{example}
  $$
  \sqrt{n}=\Omega(\lg n) \quad\left(c=1, n_{0}=16\right)
  $$
\end{example}
\subsection{$\Theta-$notation(tight bounds)}
\begin{definition}[$\Theta-$notation(tight bounds)]
  $$
\Theta(g(n))=\mathrm{O}(g(n)) \cap \Omega(g(n))
$$
\end{definition}
\begin{example}
  $$
  \frac{1}{2} n^{2}-2 n=\Theta\left(n^{2}\right)
  $$
\end{example}
\subsection{$o-$notation and $\omega$-notation}
$O$-notation and $\Omega$-notation are like $\leq$ and $\geq$. $o$-notation and $\omega$-notation are like $<$ and $>$.
\begin{definition}[$o-$notation]
  $o(g(n))=\{f(n):$ for any constant $c>0$ ,there is a constant $n_{0}>0$ such that $0 \leq f(n)<c g(n)$ for all $\left.n \geq n_{0}\right\}$ 
\end{definition}
\begin{example}
  $$
2 n^{2}=o\left(n^{3}\right) \quad\left(n_{0}=2 / c\right)
$$
\end{example}
\begin{definition}[$\omega$-notation]
  $o(g(n))=\{f(n):$ for any constant $c>0$ , there is a constant $n_{0}>0$
  Such that $0 \leq f(n)<\operatorname{cg}(n)$
  for all $\left.n \geq n_{0}\right\}$
\end{definition}
\begin{example}
  $$
  \sqrt{n}=\omega(\lg n) \quad\left(n_{0}=1+1 / c\right)
  $$
\end{example}
\section{Solving recurrences}

The analysis of merge sort from Lecture 1 required us to solve a recurrence.Recurrences are like solving integrals, differential equations, etc. Learn a few tricks.

\subsection{Substitution method}
\begin{definition}[Substitution method]
  The most general method:
  \begin{enumerate}
    \item Guess the form of the solution.
    \item Verify by induction.
    \item Solve for constants.
  \end{enumerate} 
\end{definition}

\begin{example}
  $$
T(n)=4 T(n / 2)+n
$$
\end{example}
\begin{itemize}
  \item Assume that $T(1)=\Theta(1)$ .
  \item Guess $O\left(n^{3}\right)$. (Prove $O$ and $\Omega$ separately.)
  \item Assume that $T(k) \leq c k^{3}$ for $k<n$
  \item Prove $T(n) \leq c n^{3}$ by induction. 
\end{itemize}
\begin{solution}
  $$
  \begin{aligned}
  T(n) &=4 T(n / 2)+n \\
  & \leq 4 c(n / 2)^{3}+n \\
  &=(c / 2) n^{3}+n \\
  &=c n^{3}-\left((c / 2) n^{3}-n\right) \leftarrow \text { desired - residual }\\
  & \leq c n^{3} \longleftarrow \text { desired }
  \end{aligned}
  $$

  whenever $(c / 2) n^{3}-n \geq 0$ , for example,if $c \geq 2$ and $n \geq 1$
\end{solution} 
\begin{note}
  We must also handle the initial conditions, that is, ground the induction with base cases.
\end{note}
\begin{enumerate}
  \item \textbf{Base}: $T(n)=\Theta(1)$ for all $n<n_{0}$, where $n_{0}$ is a suitable constant.
  \item For $1 \leq n<n_{0}$, we have  $" \Theta(1) " \leq c n^{3}$, if we pick $c$ big enough.
\end{enumerate}

\begin{center}
\textbf{This bound is not tight!}
\end{center}
\begin{note}
  A tighter upper bound? We shall prove that $T(n)=O\left(n^{2}\right)$
\end{note}
Assume that $T(k) \leq c k^{2}$ for $k<n$ :
$$
\begin{aligned}
T(n) &=4 T(n / 2)+n \\
& \leq 4 c(n / 2)^{2}+n \\
&=c n^{2}+n \\
&=O\left(n^{2}\right)
\end{aligned}
$$
\begin{center}
  \textbf{Wrong! We must prove the I.H.($T(k) \leq ck^2$)}
\end{center}
$$
=c n^{2}-(-n)
$$
for no choice of $c>0$. Lose!
\begin{note}
  \text{IDEA:} Strengthen the inductive hypothesis.(Subtract a low-order term.)
  $$
\text { Inductive hypothesis: } T(k) \leq c_{1} k^{2}-c_{2} k \text { for } k<n \text {. }
$$
\end{note}
\begin{solution}
  $$
  \begin{aligned}
  T(n) &=4 T(n / 2)+n \\
  &=4\left(c_{1}(n / 2)^{2}-c_{2}(n / 2)\right)+n \\
  &=c_{1} n^{2}-2 c_{2} n+n \\
  &=c_{1} n^{2}-c_{2} n-\left(c_{2} n-n\right) \\
  & \leq c_{1} n^{2}-c_{2} n \text { if } c_{2} \geq 1 .
  \end{aligned}
  $$
Pick $c_1$ big enough to handle the initial conditions.
\end{solution}
\subsection{Recursion-tree method}
\begin{itemize}
  \item A recursion tree models the costs(time)ofa recursive execution of an algorithm.

  \item The recursion-tree method can be unreliable, just like any method that uses ellipses (...) 
  
  \item The recursion-tree method promotes intuition, however.
  \item The recursion tree method is good for generating guesses for the substitution method.
\end{itemize}
\begin{example}
  Solve 
  $$
  T(n)=T(n / 4)+T(n / 2)+n^{2}
  $$
  %todo recursion-tree-pic
\end{example}
\subsection{The master method}
The master method applies to recurrences of the form
$$
T(n)=a T(n / b)+f(n)
$$

where $a \geq 1, b>1$ and $f$ is asymptotically positive.
\begin{theorem}[Three common cases]
  Compare $f(n)$ with $n^{\log _{b} a}$ :
  \begin{enumerate}
    \item $f(n)=O\left(n^{\log _{b} a-\varepsilon}\right)$ for some constant $\varepsilon>0$
    \begin{itemize}
      \item $f(n)$ grows polynomially slower than $n^{\log _{b} a}$ (by an $n^{\varepsilon}$ factor).
    \end{itemize}
    \textbf{Solution:}$$
    T(n)=\Theta\left(n^{\log _{b} a}\right)
    $$
    \item $f(n)=\Theta\left(n^{\log _{b} a} \lg ^{k} n\right)$ for some constant $k \geq 0$
    \begin{itemize}
      \item $f(n)$ and $n^{\log _{b} a}$ grow at similar rates.
    \end{itemize}
    \textbf{Solution:}
    $$
    T(n)=\Theta\left(n^{\log _{b} a} \lg ^{k+1} n\right)
    $$
    \item $f(n)=\Omega\left(n^{\log _{b} a+\varepsilon}\right)$ for some constant $\varepsilon>0$
    \begin{itemize}
      \item $f(n)$ grows polynomially faster than $n^{\log _{b} a}$ (by an $n^{\varepsilon}$ factor),and $f(n)$ satisfies the regularity condition that $a f(n / b) \leq c f(n)$ for some constant $c<1$
    \end{itemize}
    \textbf{Solution:}
    $$
    T(n)=\Theta(f(n))
    $$ 
  \end{enumerate}
\end{theorem}
\begin{example}
  $$
T(n)=4 T(n / 2)+n^{3}
$$
\end{example}
\begin{solution}
  $$a=4, b=2 \Rightarrow n^{\log _{b} a}=n^{2} ; f(n)=n^{3}$$

  \textbf{CASE 3}: $f(n)=\Omega\left(n^{2+\varepsilon}\right)$ for $\varepsilon=1$ and $4(n / 2)^{3} \leq c n^{3}$ (reg. cond.) for $c=1 / 2$.
  $$
\therefore T(n)=\Theta\left(n^{3}\right)
$$
\end{solution}
\begin{example}
  $$
  T(n)=4 T(n / 2)+n^{2} / \lg n
  $$
\end{example}
\begin{solution}
  $$a=4, b=2 \Rightarrow n^{\log _{b} a}=n^{2} ; f(n)=n^{2} / \lg n$$

  \textbf{Master method does not apply.} In particular, for every constant $\varepsilon>0$, we have $n^{\varepsilon}=\omega(\lg n)$.

\end{solution}

\subsubsection{Idea of master theorem}
%todo recursion-tree-master-theorem'

\end{document}